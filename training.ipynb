{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"Training\").getOrCreate()\n",
    "\n",
    "# Define data path\n",
    "BASE_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "TRAINING_DATASET_PATH = os.path.join(DATA_PATH, 'training')\n",
    "MODEL_PATH = os.path.join(BASE_PATH, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(data: pyspark.sql.DataFrame):\n",
    "    # Creates an ML Pipeline and appropriate Param Map for Cross Validation \n",
    "    \n",
    "    \n",
    "    #Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
    "\n",
    "    # Automatically identify categorical features, and index them.\n",
    "    # Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    featureIndexer =\\\n",
    "        VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "    # Train a RandomForest model.\n",
    "    rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "    # Convert indexed labels back to original labels.\n",
    "    labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                                   labels=labelIndexer.labels)\n",
    "    \n",
    "    paramMap = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [5, 10, 15, 20]) \\\n",
    "        .build()\n",
    "    \n",
    "    return (Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter]), paramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pipeline: pyspark.ml.Pipeline, \n",
    "              paramGrid, evaluator, \n",
    "              n_folds: int, \n",
    "              dataset: pyspark.sql.DataFrame, \n",
    "              model_path):\n",
    "    # Creates a cross validation evaluator according to defined pipeline and param maps.\n",
    "    # Finds the best model according to the cross validation results\n",
    "    # Persists the model to the given model path\n",
    "    model = CrossValidator(estimator=pipeline,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           numFolds=n_folds)\n",
    "    cv_model = model.fit(dataset)\n",
    "    model = cv_model.bestModel\n",
    "    evaluation = list(zip(cv_model.avgMetrics, paramGrid))\n",
    "    model.write().overwrite().save(model_path)\n",
    "    return model, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(training_data_path: str, model_save_path: str):\n",
    "    # Given training data path and model path\n",
    "    # Produces and persists an ML Model based on training data and defined configuration\n",
    "    \n",
    "    # Load training dataset\n",
    "    training_df = spark.read.parquet(TRAINING_DATASET_PATH)\n",
    "    \n",
    "    training_df.printSchema()\n",
    "    print(training_df.head(1))\n",
    "    \n",
    "    # Get pipeline configuration\n",
    "    pipeline, paramMap = get_pipeline(training_df)\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    num_folds = 2\n",
    "    \n",
    "    # Create and persist ML Model based on training data and pipeline configuration\n",
    "    model, evaluations = get_model(pipeline=pipeline,\n",
    "                                   paramGrid=paramMap,\n",
    "                                   evaluator=evaluator,\n",
    "                                   n_folds=num_folds,\n",
    "                                   dataset=training_df,\n",
    "                                   model_path=os.path.join(BASE_PATH, 'model'))\n",
    "    return model, evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "[Row(label=0.0, features=SparseVector(692, {98: 70.0, 99: 255.0, 100: 165.0, 101: 114.0, 126: 122.0, 127: 253.0, 128: 253.0, 129: 253.0, 130: 120.0, 154: 165.0, 155: 253.0, 156: 253.0, 157: 253.0, 158: 234.0, 159: 52.0, 182: 99.0, 183: 253.0, 184: 253.0, 185: 253.0, 186: 253.0, 187: 228.0, 188: 26.0, 208: 60.0, 209: 168.0, 210: 238.0, 211: 202.0, 212: 174.0, 213: 253.0, 214: 253.0, 215: 253.0, 216: 127.0, 234: 91.0, 235: 81.0, 236: 1.0, 237: 215.0, 238: 128.0, 239: 28.0, 240: 12.0, 241: 181.0, 242: 253.0, 243: 253.0, 244: 175.0, 245: 3.0, 261: 18.0, 262: 204.0, 263: 253.0, 264: 77.0, 269: 7.0, 270: 253.0, 271: 253.0, 272: 253.0, 273: 54.0, 288: 54.0, 289: 248.0, 290: 253.0, 291: 253.0, 292: 143.0, 297: 1.0, 298: 127.0, 299: 253.0, 300: 253.0, 301: 188.0, 316: 104.0, 317: 253.0, 318: 253.0, 319: 253.0, 320: 20.0, 326: 81.0, 327: 249.0, 328: 253.0, 329: 191.0, 344: 192.0, 345: 253.0, 346: 253.0, 347: 218.0, 348: 5.0, 355: 203.0, 356: 253.0, 357: 208.0, 358: 21.0, 371: 56.0, 372: 237.0, 373: 253.0, 374: 250.0, 375: 100.0, 383: 104.0, 384: 253.0, 385: 253.0, 386: 75.0, 399: 76.0, 400: 253.0, 401: 253.0, 402: 224.0, 411: 119.0, 412: 253.0, 413: 253.0, 414: 75.0, 427: 80.0, 428: 253.0, 429: 253.0, 430: 103.0, 438: 4.0, 439: 241.0, 440: 253.0, 441: 218.0, 442: 32.0, 455: 213.0, 456: 253.0, 457: 253.0, 458: 103.0, 466: 125.0, 467: 253.0, 468: 253.0, 469: 191.0, 483: 213.0, 484: 253.0, 485: 253.0, 486: 103.0, 493: 3.0, 494: 176.0, 495: 253.0, 496: 253.0, 497: 135.0, 511: 213.0, 512: 253.0, 513: 253.0, 514: 103.0, 520: 9.0, 521: 162.0, 522: 253.0, 523: 253.0, 524: 226.0, 525: 37.0, 539: 179.0, 540: 253.0, 541: 253.0, 542: 135.0, 547: 46.0, 548: 157.0, 549: 253.0, 550: 253.0, 551: 253.0, 552: 63.0, 567: 23.0, 568: 188.0, 569: 253.0, 570: 249.0, 571: 179.0, 572: 179.0, 573: 179.0, 574: 179.0, 575: 233.0, 576: 253.0, 577: 253.0, 578: 233.0, 579: 156.0, 580: 10.0, 596: 51.0, 597: 235.0, 598: 253.0, 599: 253.0, 600: 253.0, 601: 253.0, 602: 253.0, 603: 253.0, 604: 251.0, 605: 232.0, 606: 120.0, 625: 16.0, 626: 124.0, 627: 253.0, 628: 253.0, 629: 253.0, 630: 253.0, 631: 152.0, 632: 104.0}))]\n",
      "Best Model: \n",
      "Random Forest model with 15 trees.\n",
      "Saved to /Users/orz/projects/iguazio/model_example/model\n",
      "\n",
      "Cross Validated models and average results:\n",
      "Model: Random Forest model with 5 trees\t\tEvaluation: 0.9693877551020409\n",
      "Model: Random Forest model with 10 trees\t\tEvaluation: 0.9795918367346939\n",
      "Model: Random Forest model with 15 trees\t\tEvaluation: 0.9897959183673469\n",
      "Model: Random Forest model with 20 trees\t\tEvaluation: 0.9712773998488284\n"
     ]
    }
   ],
   "source": [
    "model, evaluations = training(training_data_path=TRAINING_DATASET_PATH,\n",
    "                              model_save_path=MODEL_PATH)\n",
    "\n",
    "rf = model.stages[2]\n",
    "print(f'Best Model: \\n'\\\n",
    "      f'Random Forest model with {rf.getNumTrees} trees.\\n'\\\n",
    "      f'Saved to {MODEL_PATH}\\n\\n'\\\n",
    "      'Cross Validated models and average results:')\n",
    "\n",
    "for evaluation, params in evaluations:\n",
    "    print(f'Model: Random Forest model with {list(params.values())[0]} trees\\t\\tEvaluation: {evaluation}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
